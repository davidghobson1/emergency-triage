{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping PubMed Central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Bio import Entrez\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "from json import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = ''\n",
    "Entrez.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article title\n",
    "def get_title(article_obj):\n",
    "    return article_obj['front']['article-meta']['title-group']['article-title']\n",
    "\n",
    "# get keywords\n",
    "def get_keywords(article_obj):\n",
    "    kwd_group = article_obj['front']['article-meta']['kwd-group']\n",
    "    if len(kwd_group) > 1:\n",
    "        print('Length of keyword group is larger than 1 (probably need to re-implement this function)!')\n",
    "        return np.nan\n",
    "    return article_obj['front']['article-meta']['kwd-group'][0]['kwd'] if kwd_group != [] else []\n",
    "\n",
    "# get author names\n",
    "def get_authors(article_obj):\n",
    "    contrib_group = article_obj['front']['article-meta']['contrib-group']\n",
    "    if len(contrib_group) > 1:\n",
    "        print('Length of author contribution group is larger than 1 (probably need to re-implement this function)!')\n",
    "        return np.nan\n",
    "    author_objs = contrib_group[0]['contrib']\n",
    "    authors = []\n",
    "    for author_obj in author_objs:\n",
    "        if len(author_obj['name']) > 0:\n",
    "            if len(author_obj['name']) > 1:\n",
    "                print(\"Author has more than 1 name (need to re-implement this function)!\")\n",
    "                return np.nan\n",
    "            author = author_obj['name'][0]\n",
    "            authors.append(f\"{author['given-names']} {author['surname']}\")\n",
    "    return authors\n",
    "\n",
    "# get abstract\n",
    "def get_abstract(article_obj):\n",
    "    abstract_objs = article_obj['front']['article-meta']['abstract']\n",
    "    if len(abstract_objs) == 0:\n",
    "        return np.nan\n",
    "    abstract = \"\"\n",
    "    for abstract_obj in abstract_objs:\n",
    "        abstract += get_section_text(abstract_obj)\n",
    "    return abstract\n",
    "\n",
    "# get the PMC ID (PubMed Central ID)\n",
    "def get_pmc_id(article_obj):\n",
    "    article_ids = article_obj['front']['article-meta']['article-id']\n",
    "    for id_ in article_ids:\n",
    "        if id_.attributes['pub-id-type'] == 'pmc':\n",
    "            return str(id_)\n",
    "    return \"N/A\"\n",
    "\n",
    "# get the PubMed ID\n",
    "def get_pm_id(article_obj):\n",
    "    article_ids = article_obj['front']['article-meta']['article-id']\n",
    "    for id_ in article_ids:\n",
    "        if id_.attributes['pub-id-type'] == 'pmid':\n",
    "            return str(id_)\n",
    "    return \"N/A\"\n",
    "\n",
    "# get article full-text\n",
    "def get_full_text(article_obj):\n",
    "    # checks if full-text is available\n",
    "    if 'body' not in article_obj:\n",
    "        return \"N/A\"\n",
    "    return get_section_text(article_obj['body'])\n",
    "\n",
    "# get section text by recursively traversing the Entrez article objects\n",
    "def get_section_text(text_body):\n",
    "    full_text = \"\"\n",
    "    if 'title' in text_body:\n",
    "        full_text += \"\\t\" + text_body['title'] + '\\n'\n",
    "    if 'p' in text_body:\n",
    "        for paragraph in text_body['p']:\n",
    "            full_text += paragraph + '\\n'\n",
    "    if 'sec' in text_body:\n",
    "        for section in text_body['sec']:\n",
    "            full_text += get_section_text(section) + '\\n'\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the article objects from PubMed based on the query and retstart.\n",
    "def query_pubmed(search_query, db='pmc', rettype='xml', retstart=0, retmax=100):\n",
    "    # get the article IDs\n",
    "    handle_esearch = Entrez.esearch(db=db, term=search_query, sort='relevance', retstart=retstart, retmax=retmax)\n",
    "    results_esearch = Entrez.read(handle_esearch)\n",
    "\n",
    "    ids_ = results_esearch['IdList']\n",
    "    efetch_query = \",\".join(ids_)\n",
    "\n",
    "    # get the full text articles\n",
    "    handle_efetch = Entrez.efetch(id=efetch_query, db=db, rettype=rettype, retmax=retmax)\n",
    "    article_objs = Entrez.read(handle_efetch, validate=False)\n",
    "    \n",
    "    return article_objs, ids_\n",
    "\n",
    "# separates meaningful article objects returned from PubMed from unmeaningful ones\n",
    "# here, meaningful means that the article object possesses its PMC id\n",
    "# if this is not the case, it will contain garbage info\n",
    "def filter_article_objects(article_objs, ids_):\n",
    "    good_articles, bad_ids = [], [] \n",
    "    for i, article_obj in enumerate(article_objs):\n",
    "        pmc_id = get_pmc_id(article_obj)\n",
    "        if pmc_id == ids_[i]:\n",
    "            good_articles.append(article_obj)\n",
    "        else:\n",
    "            bad_ids.append(ids_[i])\n",
    "    print(f\"\\tNumber bad articles: {len(bad_ids)}\")\n",
    "    return good_articles, bad_ids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biomedical NER functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the BERN2 web API\n",
    "BERN2_CHAR_LIMIT = 5000         # BERN2 has a 5000 char limit\n",
    "\n",
    "# query the BERN2 web API with the given text\n",
    "def bern2_query_plain(text, url=\"http://bern2.korea.ac.kr/plain\"):\n",
    "    return requests.post(url, json={'text': text}).json()\n",
    "\n",
    "# retrieve the MeSH ID from the BERN2 results\n",
    "def get_mesh_id(disease_obj):\n",
    "    ids = disease_obj['id']\n",
    "    for id_ in ids:\n",
    "        if id_[:4] == 'mesh':\n",
    "            return re.sub(\"mesh:\", \"\", id_)\n",
    "    return 'N/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comorbidity identification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple comorbidity identification function\n",
    "# if two disease entities are separated by the word \"comorbid\" in the same sentence,\n",
    "# the diseases are considered a comorbid pair\n",
    "def simple_search(doc, diseases):\n",
    "    results = []\n",
    "    if len(diseases) != 0:\n",
    "        disease_idx = 0\n",
    "        for sent in doc.sents:\n",
    "            sent_ents = list(sent.ents)\n",
    "            if len(sent_ents) == 0:\n",
    "                continue\n",
    "            elif len(sent_ents) == 1 and sent_ents[0].label_ == 'DISEASE':\n",
    "                disease_idx += 1\n",
    "                continue\n",
    "            for i in range(len(sent_ents) - 1):\n",
    "                if sent_ents[i].label_ != 'DISEASE':\n",
    "                    continue\n",
    "                ent1_start = sent_ents[i].start_char\n",
    "                ent2_end = sent_ents[i+1].end_char\n",
    "                if sent_ents[i+1].label_ == 'DISEASE' and 'comorbid' in doc.text[ent1_start:ent2_end]:\n",
    "                    mesh_1 = get_mesh_id(diseases[disease_idx])\n",
    "                    mesh_2 = get_mesh_id(diseases[disease_idx+1])\n",
    "                    if mesh_1 != mesh_2:       \n",
    "                        hit = {\n",
    "                            'span': sent,\n",
    "                            'comorbidity': (sent_ents[i].text, sent_ents[i+1].text),\n",
    "                            'comorbidity_mesh': (mesh_1, mesh_2)\n",
    "                        }\n",
    "                        results.append(hit)\n",
    "                disease_idx += 1\n",
    "            if sent_ents[-1].label_ == 'DISEASE':\n",
    "                disease_idx += 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding comorbidities and processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comorbidities_text(text, finding_scheme=simple_search):\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    \n",
    "    comorbid_sentences = get_comorbid_sentences(text)\n",
    "    \n",
    "    doc, diseases = get_bern2_docs(\" \".join(comorbid_sentences))\n",
    "    \n",
    "    return finding_scheme(doc, diseases)\n",
    "    \n",
    "# clean raw text from scraping results for further processing\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"<xref.+?</xref>|<.+?>\", \"\", text)            # remove references completely, and HTML formatting tags\n",
    "    text = text.encode('ascii', errors='ignore').decode()       # convert to ASCII (BERN2 requires this)\n",
    "    text = re.sub(\"\\s+\", \" \", text).strip()                     # remove any extraneous formatting characters\n",
    "    return text\n",
    "\n",
    "# find sentences with the word \"comorbid\" in them\n",
    "def get_comorbid_sentences(text): \n",
    "    article_doc = nlp(text, disable=['attribute_ruler', 'lemmatizer', 'NER']) \n",
    "    comorbid_sentences = []\n",
    "    for sent in article_doc.sents:\n",
    "        if 'comorbid' in sent.text:\n",
    "            comorbid_sentences.append(sent.text)  \n",
    "    return comorbid_sentences\n",
    " \n",
    "def get_bern2_docs(text):  \n",
    "    doc = nlp(text)                                 # apply spaCy's NLP\n",
    "    # split the doc if the text size is too large\n",
    "    if len(doc.text) > BERN2_CHAR_LIMIT:\n",
    "        batches = batchify_doc(doc)                 # batch the doc\n",
    "        bern2_result = {'annotations': []}\n",
    "        idx_offset = 0\n",
    "        for batch in batches:\n",
    "            if not len(batch) > BERN2_CHAR_LIMIT:\n",
    "                batch_text, batch_diseases = get_bern2_docs(batch)\n",
    "                for disease in batch_diseases:\n",
    "                    disease['span']['begin'] = disease['span']['begin'] + idx_offset \n",
    "                    disease['span']['end'] = disease['span']['end'] + idx_offset \n",
    "                    bern2_result['annotations'].append(disease)\n",
    "                idx_offset += len(batch_text.text) + 1\n",
    "            else:\n",
    "                print(\"Caution here!\")\n",
    "                # sometimes the spaCy Doc doesn't parse the sentences correctly and still gives sentences greater\n",
    "                # than 5000 characters so we remove those \n",
    "                idx_offset += len(batch_text.text) + 1\n",
    "    else:\n",
    "        # query BERN2 and error check\n",
    "        try:\n",
    "            bern2_result = bern2_query_plain(doc.text)\n",
    "        except(ValueError):\n",
    "            print(\"Error in querying BERN2: BERN2 got a JSON decoding error\")\n",
    "            return doc, []\n",
    "        if 'error_message' in bern2_result:\n",
    "            print(f\"Error in querying BERN2: {bern2_result['error_message']}\")\n",
    "            return doc, []\n",
    "        elif 'annotations' not in bern2_result:\n",
    "            print(\"Error in querying BERN2: No error but no annotations given.\")\n",
    "            return doc, []\n",
    "    \n",
    "    doc, diseases = add_diseases_to_doc(doc, bern2_result)\n",
    "    \n",
    "    return doc, diseases\n",
    "\n",
    "def add_diseases_to_doc(doc, bern2_result):\n",
    "\n",
    "    annotations = bern2_result['annotations']\n",
    "\n",
    "    # filter the diseases\n",
    "    diseases = []\n",
    "    for result in annotations:\n",
    "        if result['obj'] == 'disease' and ('comorbid' not in result['mention']) and (result['mention'] not in ['medical', 'violent crimes']):\n",
    "            diseases.append(result)\n",
    "\n",
    "    # create a list of Span objects for the diseases\n",
    "    disease_ents = [doc.char_span(disease['span']['begin'], disease['span']['end'], label='DISEASE') for disease in diseases]\n",
    "    \n",
    "    # sometimes BERN2 mis-identifies a disease. Remove those\n",
    "    diseases = [diseases[i] for i in range(len(diseases)) if disease_ents[i] is not None]\n",
    "    disease_ents = [disease_ent for disease_ent in disease_ents if disease_ent is not None]\n",
    "    \n",
    "    # update the entities with the diseases\n",
    "    doc.set_ents(disease_ents, default='unmodified')\n",
    "    \n",
    "    return doc, diseases\n",
    "    \n",
    "# batch the Doc object\n",
    "# only give back results less than the limit number of characters\n",
    "def batchify_doc(doc, limit=BERN2_CHAR_LIMIT):\n",
    "    batches = []\n",
    "    curr_batch = \"\"\n",
    "    for sent in doc.sents:\n",
    "        if len(\" \".join([curr_batch, sent.text])) > limit:\n",
    "            batches.append(curr_batch)\n",
    "            curr_batch = \"\"\n",
    "        curr_batch = \" \".join([curr_batch, sent.text]) if curr_batch != \"\" else sent.text    \n",
    "    batches.append(curr_batch)    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find comorbdities from an article object\n",
    "def find_comorbidities(article_obj, sleep_time=0.1):\n",
    "    if len(get_full_text(article_obj)) > 1000000:\n",
    "        print(f'\\tArticle PMC ID {get_pmc_id(article_obj)} is too long for spaCy')\n",
    "        return []\n",
    "    comorbid_results = find_comorbidities_text(get_full_text(article_obj))\n",
    "    sleep(sleep_time)\n",
    "    article_results = []\n",
    "    if len(comorbid_results) != 0:\n",
    "        print(f\"\\t\\tGot {len(comorbid_results)} results!\")\n",
    "        for result in comorbid_results:\n",
    "            mesh_1 = result['comorbidity_mesh'][0]\n",
    "            mesh_2 = result['comorbidity_mesh'][1]\n",
    "            comorbidity_mesh = \" \".join([mesh_1, mesh_2]) if mesh_1 < mesh_2 else \" \".join([mesh_2, mesh_1])\n",
    "            article_results.append({\n",
    "                'title': get_title(article_obj),\n",
    "                'pmc_id': get_pmc_id(article_obj),\n",
    "                'sentence': result['span'].text,\n",
    "                'disease_1': result['comorbidity'][0],\n",
    "                'disease_2': result['comorbidity'][1],\n",
    "                'mesh_1': mesh_1,\n",
    "                'mesh_2': mesh_2,\n",
    "                'comorbidity_mesh': comorbidity_mesh\n",
    "            })\n",
    "    return article_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, missed_ids, bad_article_ids, no_comorbids_ids, retstart):\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    good_ids = list(df['pmc_id'].unique()) if len(results) != 0 else []\n",
    "\n",
    "    ids_scraped = {\n",
    "        'Successfully Scraped': {\n",
    "            'Comorbidities Found': good_ids,\n",
    "            'No Comorbidities Found': no_comorbids_ids\n",
    "        }, \n",
    "        'Errors': {\n",
    "            'EFetch Retrieval Errors': missed_ids,\n",
    "            'EFetch Parsing Errors': bad_article_ids,   \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_filepath = f\"results_{retstart}.csv\"\n",
    "    pmc_ids_filepath = f\"ids_{retstart}.json\"\n",
    "    \n",
    "    df.to_csv(results_filepath, index=False)\n",
    "    with open(pmc_ids_filepath, 'w') as f:\n",
    "        dump(ids_scraped, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'open access[filter] elderly comorbidities'\n",
    "num_articles = 10000\n",
    "start = 0\n",
    "\n",
    "batch_size = 100\n",
    "save_interval = batch_size*10\n",
    "max_tries_bern2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missed_ids = []             # EFetch couldn't fetch articles\n",
    "bad_article_ids = []        # EFetch results couldn't be parsed\n",
    "no_comorbids_ids = []       # articles had no comorbidity information\n",
    "results = []                # comorbidity results\n",
    "for retstart in range(start, num_articles, batch_size):\n",
    "    \n",
    "    # save file if enough articles have been processed\n",
    "    if retstart != start and (retstart % save_interval == 0):\n",
    "        print(f\"Saving file: {retstart}\")\n",
    "        save_results(results, missed_ids, bad_article_ids, no_comorbids_ids, retstart)\n",
    "        results, missed_ids, bad_article_ids, no_comorbids_ids = [], [], [], []\n",
    "        \n",
    "    retmax = batch_size if num_articles - retstart > batch_size else num_articles - retstart \n",
    "    article_objs, ids_ = query_pubmed(search_query, retstart=retstart, retmax=retmax)\n",
    "    \n",
    "    # skip if pubmed query returned nothing\n",
    "    if article_objs == []:\n",
    "        missed_ids += ids_\n",
    "        continue\n",
    "        \n",
    "    # filter article objects which aren't of good quality\n",
    "    article_objs, bad_ids = filter_article_objects(article_objs, ids_)\n",
    "    bad_article_ids += bad_ids\n",
    "    \n",
    "    # process the articles\n",
    "    for i, article_obj in enumerate(article_objs):\n",
    "        print(f\"\\tProcessing article {i + retstart}\")\n",
    "        for j in range(max_tries_bern2):\n",
    "            try:\n",
    "                comorbid_results = find_comorbidities(article_obj, sleep_time=0)\n",
    "                if comorbid_results != []:\n",
    "                    results += comorbid_results\n",
    "                else:\n",
    "                    no_comorbids_ids.append(get_pmc_id(article_obj))\n",
    "                break\n",
    "            except(ConnectionError, ConnectionResetError):\n",
    "                print(f\"\\t\\tRemote Disconnect Error\\n\\t\\tSleeping and trying again (tries = {j+1})...\")\n",
    "                sleep(10)\n",
    "save_results(results, missed_ids, bad_article_ids, no_comorbids_ids, num_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
